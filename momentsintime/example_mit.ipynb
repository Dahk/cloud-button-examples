{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cloudbutton Moments in Time dataset example\n",
    "## Video/image prediction\n",
    "In this notebook we will process video clips from the MiT dataset at scale with the Cloudbutton toolkit,  \n",
    "by predicting its top 5 actions with a pretrained ResNet50 neural network model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pprint\n",
    "import builtins\n",
    "import operator\n",
    "import functools\n",
    "import torch.optim\n",
    "import torch.nn.parallel\n",
    "from torch import save, load\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from utils import extract_frames\n",
    "from models import load_model, load_transform, load_categories\n",
    "\n",
    "from cloudbutton import Pool\n",
    "from cloudbutton.util import get_uuid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backends\n",
    "The same program can be run in a local environtment with processes or executed by\n",
    "functions in the cloud. After we choose a backend, only a few file locations must\n",
    "be changed. In this example we will be using the cloud functions backend.\n",
    "\n",
    "We will be using a custom runtime for our functions which has torch, torchvision,\n",
    "ffmpeg and opencv-python modules already installed.\n",
    "We will store the pretrained weights in the cloud so that functions can access it.\n",
    "Then, after functions get the models weights they will start preprocessing input\n",
    "videos and inferring them one by one.\n",
    "  \n",
    "Later in this notebook, we will see a little improvement detail to this process.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_EXEC = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DATA_DIR = 'momentsintime/input_data'\n",
    "\n",
    "if LOCAL_EXEC:\n",
    "    import os\n",
    "    from builtins import open\n",
    "    initargs = {\n",
    "        'backend': 'localhost',\n",
    "        'storage_backend': 'localhost'\n",
    "        }\n",
    "    weights_location = '/dev/shm/model_weights'\n",
    "    INPUT_DATA_DIR = os.path.abspath(INPUT_DATA_DIR)\n",
    "\n",
    "else:\n",
    "    import cloud_proxy as os\n",
    "    from cloud_proxy import open\n",
    "    initargs = {\n",
    "        'backend': 'ibm_cf',\n",
    "        'storage_backend': 'ibm_cos',\n",
    "        'runtime': 'dhak/pywren-runtime-pytorch:3.6',\n",
    "        'runtime_memory': 2048\n",
    "        }\n",
    "    weights_location = 'momentsintime/models/model_weights'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_locations = [os.path.join(INPUT_DATA_DIR, name) for name in os.listdir(INPUT_DATA_DIR)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we have masked the `open` function and `os` module with a proxy\n",
    "to manage files from the cloud transparently.  \n",
    "We will use `builtins.open` from now on to explicitly access a local file as some accesses have to occur in the very same machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download pretrained ResNet50 model weights and save them in a directory accessible by all functions (`weights_location`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_URL = 'http://moments.csail.mit.edu/moments_models'\n",
    "WEIGHTS_FILE = 'moments_RGB_resnet50_imagenetpretrained.pth.tar'\n",
    "\n",
    "if not os.access(WEIGHTS_FILE, os.R_OK):\n",
    "    os.system('wget ' + '/'.join([ROOT_URL, WEIGHTS_FILE]))\n",
    "\n",
    "with builtins.open(WEIGHTS_FILE, 'rb') as f_in:\n",
    "    weights = f_in.read()\n",
    "with open(weights_location, 'wb') as f_out:\n",
    "    f_out.write(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video prediction function code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SEGMENTS = 16\n",
    "\n",
    "# Get dataset categories\n",
    "categories = load_categories()\n",
    "\n",
    "# Load the video frame transform\n",
    "transform = load_transform()\n",
    "\n",
    "def predict_video(weights_location, video_locations):\n",
    "    with open(weights_location, 'rb') as f:\n",
    "        model = load_model(f)\n",
    "    model.eval()\n",
    "\n",
    "    results = []\n",
    "    local_video_loc = 'video_to_predict_{}.mp4'.format(get_uuid())\n",
    "\n",
    "    for video_loc in video_locations:\n",
    "        start = time.time()\n",
    "        with open(video_loc, 'rb') as f_in:\n",
    "            with builtins.open(local_video_loc, 'wb') as f_out:\n",
    "                f_out.write(f_in.read())\n",
    "\n",
    "        # Obtain video frames\n",
    "        frames = extract_frames(local_video_loc, NUM_SEGMENTS)\n",
    "\n",
    "        # Prepare input tensor [num_frames, 3, 224, 224]\n",
    "        input_v = torch.stack([transform(frame) for frame in frames])\n",
    "\n",
    "        # Make video prediction\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_v)\n",
    "            h_x = F.softmax(logits, 1).mean(dim=0)\n",
    "            probs, idx = h_x.sort(0, True)\n",
    "\n",
    "        # Output the prediction\n",
    "        output = dict(key=video_loc, predictions={})\n",
    "        for i in range(0, 5):\n",
    "            output['predictions'][categories[idx[i]]] = round(float(probs[i]), 5)\n",
    "        output['iter_duration'] = time.time() - start\n",
    "        results.append(output)\n",
    "        #os.remove(local_video_loc)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map functions\n",
    "Similar to the `multiprocessing` module API, we use a Pool to map the video keys\n",
    "across n workers (concurrency). However, we do not have to instantiate a Pool of\n",
    "n workers *specificly*, it is the map function that will invoke as many workers according\n",
    "to the length of the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONCURRENCY = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Pool(initargs=initargs) as pool:\n",
    "    iterable = [(weights_location, video_locations[n::CONCURRENCY]) \n",
    "                for n in range(CONCURRENCY) if n < len(video_locations)]\n",
    "    start = time.time()\n",
    "    results = pool.map(func=predict_video, iterable=iterable)\n",
    "    end = time.time()\n",
    "    \n",
    "print('\\nDone.')\n",
    "print('Videos processed:', len(video_locations))\n",
    "print('Total duration:', round(end - start, 2), 'sec\\n')\n",
    "\n",
    "results = functools.reduce(operator.iconcat, results, [])\n",
    "pprint.pprint(results[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------\n",
    "\n",
    "## Performance improvement\n",
    "Now, since we know every function will have to pull the model weights from\n",
    "the cloud storage, we can actually pack these weights with the runtime image\n",
    "and reduce the start-up cost substantially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initargs['runtime'] = 'dhak/pywren-runtime-resnet'\n",
    "weights_location = '/momentsintime/model_weights'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_video(weights_location, video_locations):\n",
    "    # force load weigths from local file\n",
    "    with builtins.open(weights_location, 'rb') as f:\n",
    "        model = load_model(f)\n",
    "    model.eval()\n",
    "\n",
    "    results = []\n",
    "    local_video_loc = 'video_to_predict_{}.mp4'.format(get_uuid())\n",
    "\n",
    "    for video_loc in video_locations:\n",
    "        start = time.time()\n",
    "        with open(video_loc, 'rb') as f_in:\n",
    "            with builtins.open(local_video_loc, 'wb') as f_out:\n",
    "                f_out.write(f_in.read())\n",
    "                \n",
    "        # Obtain video frames\n",
    "        frames = extract_frames(local_video_loc, NUM_SEGMENTS)\n",
    "\n",
    "        # Prepare input tensor [num_frames, 3, 224, 224]\n",
    "        input_v = torch.stack([transform(frame) for frame in frames])\n",
    "\n",
    "        # Make video prediction\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_v)\n",
    "            h_x = F.softmax(logits, 1).mean(dim=0)\n",
    "            probs, idx = h_x.sort(0, True)\n",
    "\n",
    "        # Output the prediction\n",
    "        output = dict(key=video_loc, predictions={})\n",
    "        for i in range(0, 5):\n",
    "            output['predictions'][categories[idx[i]]] = round(float(probs[i]), 5)\n",
    "\n",
    "        output['iter_duration'] = time.time() - start\n",
    "        results.append(output)\n",
    "        #os.remove(local_video_loc)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Pool(initargs=initargs) as pool:\n",
    "    iterable = [(weights_location, video_locations[n::CONCURRENCY]) \n",
    "                for n in range(CONCURRENCY) if n < len(video_locations)]\n",
    "    start = time.time()\n",
    "    results = pool.map(func=predict_video, iterable=iterable)\n",
    "    end = time.time()\n",
    "    \n",
    "print('\\nDone.')\n",
    "print('Videos processed:', len(video_locations))\n",
    "print('Total duration:', round(end - start, 2), 'sec\\n')\n",
    "\n",
    "results = functools.reduce(operator.iconcat, results, [])\n",
    "pprint.pprint(results[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.remove(WEIGHTS_FILE)\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    os.remove(weights_location)\n",
    "except FileNotFoundError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dockerfile and build scripts for both runtimes can be found in the docker/ folder.\n",
    "\n",
    "### Source of many of this programs code is from the demonstration in https://github.com/zhoubolei/moments_models\n",
    "\n",
    "### Moments in Time article: http://moments.csail.mit.edu/#paper\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python36964bit325e2e176c1e4c56af6d2bec6f3f9965"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}