{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cloudbutton Moments in Time dataset example\n",
    "## Video/image prediction\n",
    "In this notebook we will process video clips from the MiT dataset at scale with the Cloudbutton toolkit,  \n",
    "by predicting its top 5 actions with a pretrained ResNet50 neural network model and then counting how many\n",
    "occurrences of each category we have predicted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import builtins\n",
    "import torch.optim\n",
    "import torch.nn.parallel\n",
    "from torch import save, load\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from utils import extract_frames\n",
    "from models import load_model, load_transform, load_categories\n",
    "\n",
    "from cloudbutton import Pool, Queue\n",
    "from cloudbutton.util import get_uuid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backends\n",
    "The same program can be run in a local environtment with processes or executed by\n",
    "functions in the cloud. After we choose a backend, only a few file locations must\n",
    "be changed. In this example we will be using the cloud functions backend.\n",
    "\n",
    "We will be using a custom runtime for our functions which has torch, torchvision,\n",
    "ffmpeg and opencv-python modules already installed.\n",
    "We will store the pretrained weights in the cloud so that functions can access it.\n",
    "Then, after functions get the models weights they will start preprocessing input\n",
    "videos and inferring them one by one.\n",
    "  \n",
    "Later in this notebook, we will see a little improvement detail to this process.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_EXEC = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DATA_DIR = 'momentsintime/input_data'\n",
    "\n",
    "if LOCAL_EXEC:\n",
    "    import os\n",
    "    from builtins import open\n",
    "    initargs = {\n",
    "        'backend': 'localhost',\n",
    "        'storage_backend': 'localhost'\n",
    "        }\n",
    "    weights_location = '/dev/shm/model_weights'\n",
    "    INPUT_DATA_DIR = os.path.abspath(INPUT_DATA_DIR)\n",
    "\n",
    "else:\n",
    "    from cloudbutton import CloudFileProxy\n",
    "    os = CloudFileProxy()\n",
    "    open = os.open\n",
    "    initargs = {\n",
    "        'backend': 'ibm_cf',\n",
    "        'storage_backend': 'ibm_cos',\n",
    "        'runtime': 'dhak/pywren-runtime-pytorch:3.6',\n",
    "        'runtime_memory': 2040\n",
    "        }\n",
    "    weights_location = 'momentsintime/models/model_weights'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_locations = [os.path.join(INPUT_DATA_DIR, name) for name in os.listdir(INPUT_DATA_DIR)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we have masked the `open` function and `os` module with a proxy\n",
    "to manage files from the cloud transparently.  \n",
    "We will use `builtins.open` from now on to explicitly access a local file as some accesses have to occur in the very same machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download pretrained ResNet50 model weights and save them in a directory accessible by all functions (`weights_location`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_URL = 'http://moments.csail.mit.edu/moments_models'\n",
    "WEIGHTS_FILE = 'moments_RGB_resnet50_imagenetpretrained.pth.tar'\n",
    "\n",
    "if not os.access(WEIGHTS_FILE, os.R_OK):\n",
    "    os.system('wget ' + '/'.join([ROOT_URL, WEIGHTS_FILE]))\n",
    "\n",
    "with builtins.open(WEIGHTS_FILE, 'rb') as f_in:\n",
    "    weights = f_in.read()\n",
    "with open(weights_location, 'wb') as f_out:\n",
    "    f_out.write(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video prediction function code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SEGMENTS = 16\n",
    "\n",
    "# Get dataset categories\n",
    "categories = load_categories()\n",
    "\n",
    "# Load the video frame transform\n",
    "transform = load_transform()\n",
    "\n",
    "def predict_videos(queue, video_locations):\n",
    "    with open(weights_location, 'rb') as f:\n",
    "        model = load_model(f)\n",
    "    model.eval()\n",
    "\n",
    "    results = []\n",
    "    local_video_loc = 'video_to_predict_{}.mp4'.format(get_uuid())\n",
    "\n",
    "    for video_loc in video_locations:\n",
    "        start = time.time()\n",
    "        with open(video_loc, 'rb') as f_in:\n",
    "            with builtins.open(local_video_loc, 'wb') as f_out:\n",
    "                f_out.write(f_in.read())\n",
    "\n",
    "        # Obtain video frames\n",
    "        frames = extract_frames(local_video_loc, NUM_SEGMENTS)\n",
    "\n",
    "        # Prepare input tensor [num_frames, 3, 224, 224]\n",
    "        input_v = torch.stack([transform(frame) for frame in frames])\n",
    "\n",
    "        # Make video prediction\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_v)\n",
    "            h_x = F.softmax(logits, 1).mean(dim=0)\n",
    "            probs, idx = h_x.sort(0, True)\n",
    "\n",
    "        # Output the prediction\n",
    "        result = dict(key=video_loc)\n",
    "        result['prediction'] = (idx[0], round(float(probs[0]), 5))\n",
    "        result['iter_duration'] = time.time() - start\n",
    "        results.append(result)\n",
    "    queue.put(results)\n",
    "\n",
    "def reduce(queue, n):\n",
    "    pred_x_categ = {}\n",
    "    for categ in categories:\n",
    "        pred_x_categ[categ] = 0\n",
    "\n",
    "    checkpoint = 0.2\n",
    "    res_count = 0\n",
    "\n",
    "    for i in range(n):\n",
    "        results = queue.get()\n",
    "        res_count += len(results)\n",
    "        for res in results:\n",
    "            idx, prob = res['prediction']\n",
    "            pred_x_categ[categories[idx]] += 1\n",
    "\n",
    "        if i >= (N * checkpoint):\n",
    "            print('Processed {} results.'.format(res_count))\n",
    "            checkpoint += 0.2\n",
    "\n",
    "    return pred_x_categ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map functions\n",
    "Similar to the `multiprocessing` module API, we use a Pool to map the video keys\n",
    "across n workers (concurrency). However, we do not have to instantiate a Pool of\n",
    "n workers *specificly*, it is the map function that will invoke as many workers according\n",
    "to the length of the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONCURRENCY = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queue = Queue()\n",
    "pool = Pool(initargs=initargs)\n",
    "\n",
    "# Slice data keys\n",
    "N = min(CONCURRENCY, len(video_locations))\n",
    "iterable = [(queue, video_locations[n::CONCURRENCY]) \n",
    "            for n in range(N)]\n",
    "\n",
    "# Map and reduce on the go\n",
    "start = time.time()\n",
    "pool.map_async(func=predict_videos, iterable=iterable)\n",
    "pred_x_categ = reduce(queue, N)\n",
    "end = time.time()\n",
    "    \n",
    "print('\\nDone.')\n",
    "print('Videos processed:', len(video_locations))\n",
    "print('Total duration:', round(end - start, 2), 'sec\\n')\n",
    "\n",
    "for categ, count in pred_x_categ.items():\n",
    "    if count != 0:\n",
    "        print('{}: {}'.format(categ, count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------\n",
    "\n",
    "## Performance improvement\n",
    "Now, since we know every function will have to pull the model weights from\n",
    "the cloud storage, we can actually pack these weights with the runtime image\n",
    "and reduce the start-up cost substantially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initargs['runtime'] = 'dhak/pywren-runtime-resnet'\n",
    "weights_location = '/momentsintime/model_weights'\n",
    "initargs['runtime_memory'] = 2031\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_videos(queue, video_locations):\n",
    "    # force local file access on new weights_location\n",
    "    with builtins.open(weights_location, 'rb') as f:\n",
    "        model = load_model(f)\n",
    "    model.eval()\n",
    "\n",
    "    results = []\n",
    "    local_video_loc = 'video_to_predict_{}.mp4'.format(get_uuid())\n",
    "\n",
    "    for video_loc in video_locations:\n",
    "        start = time.time()\n",
    "        with open(video_loc, 'rb') as f_in:\n",
    "            with builtins.open(local_video_loc, 'wb') as f_out:\n",
    "                f_out.write(f_in.read())\n",
    "\n",
    "        # Obtain video frames\n",
    "        frames = extract_frames(local_video_loc, NUM_SEGMENTS)\n",
    "\n",
    "        # Prepare input tensor [num_frames, 3, 224, 224]\n",
    "        input_v = torch.stack([transform(frame) for frame in frames])\n",
    "\n",
    "        # Make video prediction\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_v)\n",
    "            h_x = F.softmax(logits, 1).mean(dim=0)\n",
    "            probs, idx = h_x.sort(0, True)\n",
    "\n",
    "        # Output the prediction\n",
    "        result = dict(key=video_loc)\n",
    "        result['prediction'] = (idx[0], round(float(probs[0]), 5))\n",
    "        result['iter_duration'] = time.time() - start\n",
    "        results.append(result)\n",
    "    queue.put(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queue = Queue()\n",
    "pool = Pool(initargs=initargs)\n",
    "\n",
    "# Slice data keys\n",
    "N = min(CONCURRENCY, len(video_locations))\n",
    "iterable = [(queue, video_locations[n::CONCURRENCY]) \n",
    "            for n in range(N)]\n",
    "\n",
    "# Map and reduce on the go\n",
    "start = time.time()\n",
    "r = pool.map_async(func=predict_videos, iterable=iterable)\n",
    "pred_x_categ = reduce(queue, N)\n",
    "end = time.time()\n",
    "    \n",
    "print('\\nDone.')\n",
    "print('Videos processed:', len(video_locations))\n",
    "print('Total duration:', round(end - start, 2), 'sec\\n')\n",
    "\n",
    "for categ, count in pred_x_categ.items():\n",
    "    if count != 0:\n",
    "        print('{}: {}'.format(categ, count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.remove(WEIGHTS_FILE)\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    os.remove(weights_location)\n",
    "except FileNotFoundError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dockerfile and build scripts for both runtimes can be found in the docker/ folder.\n",
    "\n",
    "### Source code adapted from the demonstration in https://github.com/zhoubolei/moments_models\n",
    "\n",
    "### Moments in Time article: http://moments.csail.mit.edu/#paper\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python36964bit325e2e176c1e4c56af6d2bec6f3f9965"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
