{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cloudbutton Moments in Time dataset example\n",
    "## Video/image prediction\n",
    "In this notebook we will process video clips from the MiT dataset at scale with the Cloudbutton toolkit,  \n",
    "by predicting its top 5 actions with a pretrained ResNet50 neural network model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pprint\n",
    "import builtins\n",
    "import operator\n",
    "import functools\n",
    "import torch.optim\n",
    "import torch.nn.parallel\n",
    "from torch import save, load\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from utils import extract_frames, CloudFileProxy\n",
    "from models import load_model, load_transform, load_categories\n",
    "\n",
    "from cloudbutton import Pool, CloudStorage\n",
    "from cloudbutton.util import get_uuid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the pretrained ResNet50 model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_URL = 'http://moments.csail.mit.edu/moments_models'\n",
    "WEIGHTS_FILE = 'moments_RGB_resnet50_imagenetpretrained.pth.tar'\n",
    "\n",
    "if not os.access(WEIGHTS_FILE, os.R_OK):\n",
    "    os.system('wget ' + '/'.join([ROOT_URL, WEIGHTS_FILE]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backends\n",
    "The same program can be run in a local environtment with processes or executed by\n",
    "functions un the cloud. After we choose a backend, only a few file locations must\n",
    "be modified. In this example we will be using the cloud functions backend.\n",
    "\n",
    "First, we have to specify the prefix in our bucket where the dataset is located.\n",
    "We will be using a custom runtime for our functions which has torch, torchvision,\n",
    "ffmpeg and opencv-python modules already installed.\n",
    "We will store the pretrained weights in the cloud so that functions can access it.\n",
    "Then, after functions get the models weights they will start preprocessing input\n",
    "videos and inferring them one by one.\n",
    "  \n",
    "Later in this notebook, we will see a little improvement detail to this process.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_EXEC = False\n",
    "INPUT_DATA_DIR = 'momentsintime/input_data'\n",
    "CONCURRENCY = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOCAL_EXEC:\n",
    "    initargs = {\n",
    "        'backend': 'localhost',\n",
    "        'storage_backend': 'localhost'\n",
    "        }\n",
    "    weights_location = '/dev/shm/' + WEIGHTS_FILE\n",
    "    video_locations = [os.path.abspath(os.path.join(INPUT_DATA_DIR, name)) \n",
    "                        for name in os.listdir(INPUT_DATA_DIR)]\n",
    "    open = builtins.open\n",
    "\n",
    "else:\n",
    "    CUSTOM_RUNTIME = 'dhak/pywren-runtime-pytorch:3.6'\n",
    "    initargs = {\n",
    "        'backend': 'ibm_cf',\n",
    "        'storage_backend': 'ibm_cos',\n",
    "        'runtime': CUSTOM_RUNTIME,\n",
    "        'runtime_memory': 1280\n",
    "        }\n",
    "    weights_location = 'momentsintime/models/' + WEIGHTS_FILE\n",
    "    cloud_storage = CloudStorage()\n",
    "    video_locations = cloud_storage.list_tmp_data(prefix=INPUT_DATA_DIR)\n",
    "    open = CloudFileProxy(cloud_storage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we have masked the `open` function with a proxy\n",
    "to access files/objects from the cloud.  \n",
    "We will use `builtins.open` from now on to explicitly access a local file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model weights to the cloud object storage / shared memory (local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with builtins.open(WEIGHTS_FILE, 'rb') as f_in:\n",
    "    with open(weights_location, 'wb') as f_out:\n",
    "        f_out.write(f_in.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SEGMENTS = 16\n",
    "\n",
    "# Get dataset categories\n",
    "categories = load_categories()\n",
    "\n",
    "# Load the video frame transform\n",
    "transform = load_transform()\n",
    "\n",
    "def predict_video(open, weights_location, video_locations):\n",
    "    with open(weights_location, 'rb') as f:\n",
    "        model = load_model(f)\n",
    "    model.eval()\n",
    "\n",
    "    results = []\n",
    "    local_video_loc = 'video_to_predict_{}.mp4'.format(get_uuid())\n",
    "\n",
    "    for video_loc in video_locations:\n",
    "        start = time.time()\n",
    "        with open(video_loc, 'rb') as f_in:\n",
    "            with builtins.open(local_video_loc, 'wb') as f_out:\n",
    "                f_out.write(f_in.read())\n",
    "\n",
    "        # Obtain video frames\n",
    "        frames = extract_frames(local_video_loc, NUM_SEGMENTS)\n",
    "\n",
    "        # Prepare input tensor [num_frames, 3, 224, 224]\n",
    "        input_v = torch.stack([transform(frame) for frame in frames])\n",
    "\n",
    "        # Make video prediction\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_v)\n",
    "            h_x = F.softmax(logits, 1).mean(dim=0)\n",
    "            probs, idx = h_x.sort(0, True)\n",
    "\n",
    "        # Output the prediction\n",
    "        output = dict(key=video_loc, predictions={})\n",
    "        for i in range(0, 5):\n",
    "            output['predictions'][categories[idx[i]]] = round(float(probs[i]), 5)\n",
    "        output['iter_duration'] = time.time() - start\n",
    "        results.append(output)\n",
    "        #os.remove(local_video_loc)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map functions\n",
    "Similar to the `multiprocessing` module API, we use a Pool to map the video keys\n",
    "across n workers (concurrency). However, we do not have to instantiate a Pool of\n",
    "n workers *specificly*, it is the map function that will invoke as many workers according\n",
    "to the length of the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Pool(initargs=initargs) as pool:\n",
    "    iterable = [(open, weights_location, video_locations[n::CONCURRENCY]) \n",
    "                for n in range(CONCURRENCY) if n < len(video_locations)]\n",
    "    start = time.time()\n",
    "    res = pool.map_async(func=predict_video, iterable=iterable)\n",
    "    results = res.get()\n",
    "    end = time.time()\n",
    "    \n",
    "print('\\nDone.')\n",
    "print('Videos processed:', len(video_locations))\n",
    "print('Total duration:', round(end - start, 2), 'sec\\n')\n",
    "\n",
    "results = functools.reduce(operator.iconcat, results, [])\n",
    "pprint.pprint(results[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------\n",
    "\n",
    "## Improvement\n",
    "Now, since we know every function will have to pull the model weights from\n",
    "the cloud storage, we can actually pack these weights with the runtime image\n",
    "and reduce the start-up cost substantially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initargs['runtime'] = 'dhak/pywren-runtime-resnet'\n",
    "weights_location = '/momentsintime/model_weights'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_video(open, weights_location, video_locations):\n",
    "    # force load weigths from local file\n",
    "    with builtins.open(weights_location, 'rb') as f:\n",
    "        model = load_model(f)\n",
    "    model.eval()\n",
    "\n",
    "    results = []\n",
    "    local_video_loc = 'video_to_predict_{}.mp4'.format(get_uuid())\n",
    "\n",
    "    for video_loc in video_locations:\n",
    "        start = time.time()\n",
    "        with open(video_loc, 'rb') as f_in:\n",
    "            with builtins.open(local_video_loc, 'wb') as f_out:\n",
    "                f_out.write(f_in.read())\n",
    "                \n",
    "        # Obtain video frames\n",
    "        frames = extract_frames(local_video_loc, NUM_SEGMENTS)\n",
    "\n",
    "        # Prepare input tensor [num_frames, 3, 224, 224]\n",
    "        input_v = torch.stack([transform(frame) for frame in frames])\n",
    "\n",
    "        # Make video prediction\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_v)\n",
    "            h_x = F.softmax(logits, 1).mean(dim=0)\n",
    "            probs, idx = h_x.sort(0, True)\n",
    "\n",
    "        # Output the prediction\n",
    "        output = dict(key=video_loc, predictions={})\n",
    "        for i in range(0, 5):\n",
    "            output['predictions'][categories[idx[i]]] = round(float(probs[i]), 5)\n",
    "\n",
    "        output['iter_duration'] = time.time() - start\n",
    "        results.append(output)\n",
    "        #os.remove(local_video_loc)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Pool(initargs=initargs) as pool:\n",
    "    iterable = [(open, weights_location, video_locations[n::CONCURRENCY]) \n",
    "                for n in range(CONCURRENCY) if n < len(video_locations)]\n",
    "    start = time.time()\n",
    "    res = pool.map_async(func=predict_video, iterable=iterable)\n",
    "    results = res.get()\n",
    "    end = time.time()\n",
    "    \n",
    "print('\\nDone.')\n",
    "print('Videos processed:', len(video_locations))\n",
    "print('Total duration:', round(end - start, 2), 'sec\\n')\n",
    "\n",
    "results = functools.reduce(operator.iconcat, results, [])\n",
    "pprint.pprint(results[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(WEIGHTS_FILE):\n",
    "    os.remove(WEIGHTS_FILE)\n",
    "\n",
    "if LOCAL_EXEC:\n",
    "    if os.path.isfile(weights_location):\n",
    "        os.remove(weights_location)\n",
    "else:\n",
    "    cloud_storage.delete_cobject(key=weights_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dockerfile and build scripts for both runtimes can be found in the docker/ folder.\n",
    "\n",
    "### Source of many of this programs code is from the demonstration in https://github.com/zhoubolei/moments_models\n",
    "\n",
    "### Moments in Time article: http://moments.csail.mit.edu/#paper\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python36964bit325e2e176c1e4c56af6d2bec6f3f9965"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
